# BERT 
## 背景
BERT是NLP领域中第一个具有通用性的预训练模型，其全称是Bidirectional Encoder Representations from Transformers。相传，改文章是一作突然某天有了一个idea，然后花了几周把代码写完实现了，发现效果还不错，就开始写文章了。
## 引言

## 相关工作

## 方法
### 模型架构参数
参数主要来自两块，一块是嵌入层，另一块是transformer块。\\
嵌入层

## 结论
把ELMO（基于双向的RNN）和GPT（基于transformer）给结合起来，变成了一个深度双向的架构，使得该预训练模型能够成功解决一系列广泛的自然语言任务。
